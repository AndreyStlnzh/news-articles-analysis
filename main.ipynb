{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блокнот для написания обработки данных (transform), полученных с newsApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возможные операции трансформации данных:\n",
    "- Очистка данных (удаление неиспользуемых признаков, дубликатов, выбросов)\n",
    "- Переформатирование (форматирование данных с разных источников. Форматы дат, валюты и тп)\n",
    "- Извлечение признаков (создание новых признаков на основе существующих)\n",
    "- Агрегация (получение необходимых показателей)\n",
    "- Объединение (объединение данных с нескольких источников)\n",
    "- Фильтрация (исключение ненужных категорий из набора данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data_apple.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Работа с пропущенными значениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[\"title\", \"content\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"author\"].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[\"author\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Была мысль заменить пустых author на Unknown, но данные записи, как видно, не хранят полезную информацию, поэтому удалим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated(subset=[\"title\", \"content\"]).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеются дубликаты, необходимо удалить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated(subset=[\"title\", \"content\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=[\"title\", \"content\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Функция очистки текста\n",
    "    Удаляет лишние пробелы и символы переноса строк и табуляции\n",
    "    Оставляет только буквы и цифры\n",
    "\n",
    "    Args:\n",
    "        text (_type_): Исходный текст\n",
    "\n",
    "    Returns:\n",
    "        _type_: Очищенный текст\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # удаляем символы табуляции и переносов строк\n",
    "    text = re.sub(r'\\[\\+\\d+ chars\\]', '', text) # удаляем [+123 chars] (есть в каждом content)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # оставляем только буквы и цифры\n",
    "    \n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)  # Токенизация\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Лемматизация\n",
    "    return \" \".join(words)\n",
    "\n",
    "data[\"title\"] = data[\"title\"].apply(clean_text)\n",
    "data[\"description\"] = data[\"description\"].apply(clean_text)\n",
    "data[\"content\"] = data[\"content\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Преобразование дат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['publishedAt'] = pd.to_datetime(data['publishedAt'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data['publishedAt'].dt.date\n",
    "data['time'] = data['publishedAt'].dt.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Фильтрация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"urlToImage\", \"source.id\", \"source.name\"], inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Какие задачи необходимо решить:\n",
    "- Подсчет самых частых слов\n",
    "- Определение тональности\n",
    "- Определение главных тем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Подсчет самых частых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(words, n: int=10):\n",
    "    word_count = Counter(words)\n",
    "    sorted_words = word_count.most_common(n)\n",
    "    return sorted_words\n",
    "\n",
    "data.head()\n",
    "\n",
    "all_words = [word for sent in data[\"content\"] for word in sent.split()]\n",
    "\n",
    "most_common = find_most_common_words(all_words, 10)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, count = zip(*most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(words, count)\n",
    "plt.title('Топ-10 самых частых слов')\n",
    "plt.xlabel('Слово')\n",
    "plt.ylabel('Частота')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Определение тональности текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "def get_text_sentiment(text: str) -> str:\n",
    "    \"\"\"Функция определения тональности текста\n",
    "\n",
    "    Args:\n",
    "        text (str): исходный текст\n",
    "\n",
    "    Returns:\n",
    "        str: тональность (neg, neu, pos, compound)\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return max(sentiment_scores, key=sentiment_scores.get)\n",
    "\n",
    "data[\"sentiment\"] = data[\"description\"].apply(get_text_sentiment)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже решил вспомнить различные варианты визуализации данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = data[\"sentiment\"].value_counts()\n",
    "\n",
    "plt.bar(sentiment_counts.index, sentiment_counts.values, color=[\"blue\", \"gray\", \"red\", \"green\"])\n",
    "plt.title('Распределение тональности текстов')\n",
    "plt.xlabel('Тональность')\n",
    "plt.ylabel('Частота')\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"sentiment\"], color=\"blue\")\n",
    "plt.title('Распределение тональности текстов')\n",
    "plt.xlabel('Тональность')\n",
    "plt.ylabel('Частота')\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(sentiment_counts, labels=sentiment_counts.index, colors=[\"blue\", \"gray\", \"red\", \"green\"])\n",
    "plt.title('Распределение тональности текстов')\n",
    "plt.xlabel('Тональность')\n",
    "plt.ylabel('Частота')\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def plot_wordcloud(sentiment_label):\n",
    "    text = \" \".join(data[data[\"sentiment\"] == sentiment_label][\"description\"])\n",
    "    wordcloud = WordCloud(width=500, height=300, background_color=\"white\").generate(text)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for {sentiment_label} News\")\n",
    "    plt.show()\n",
    "\n",
    "plot_wordcloud(\"pos\")\n",
    "plot_wordcloud(\"neg\")\n",
    "plot_wordcloud(\"neu\")\n",
    "plot_wordcloud(\"compound\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Определение главных тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=50)  # Ограничиваем топ-50 ключевых слов\n",
    "tfidf_matrix = vectorizer.fit_transform(data[\"description\"])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 5\n",
    "data[\"top_keywords\"] = tfidf_df.apply(lambda row: row.nlargest(top_n).index.tolist(), axis=1)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
